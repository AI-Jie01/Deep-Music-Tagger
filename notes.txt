- ideal goal: be able to say "in this part of the song there are elements of jazz, prog rock and a bit of grunge" :)

- define multi-output problem as following:
  -> create output label vectors so that assigned tags have decremental values based on number of assignments, i.e. gradually decrease values to each lower positioned tag
  -> predict probabilities that the song belongs to each of the (sub)set(*) of possible tags
  -> instead of taking just one value with highest probability, we take $k$ values, where $k$ is the number of originally given labels

(*) as the number of all tags is too high to be taken as the number of output classes, you could preprocess tags and assign similarities based on number of common occurences with some threshold $p$

- adjust all music samples to be more or less on the same volume level to prevent the model from learning it as a feature

- try removing voice from songs and check how does it affect classification accuracy

- if using 30s pieces turns out to have almost exact accuracy as full-length songs, try expanding the dataset by removing voice from songs that contain voice parts and serve these other parts of the songs as different samples with the same labels, of course (kick out instrumental?)
	-> anyways try different techniques to expand the dataset!

- try putting raw audio as input -> 1D convolution (may be helpful for generator - WaveNet)

- define distance measure (or similarity measure) between songs / groups

- create "informal" dataset to compare results with FMA

- feed network with more than just mel-spectrogram images?
	-> Q: how much information does mel-spectrogram contain (do faster songs end up not being converted with enough details?)?

